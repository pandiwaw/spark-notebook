{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bite sized Spark Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session & DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry point to all functionality in Spark is the `SparkSession` class. In `spark-shell`, `SparkSession` class already instantiated in `spark` object, so we can use it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// use pre initiated spark instance\n",
    "val bikeData = spark.read.option(\"header\", \"true\").csv(\"datalake/bike-data/201508_trip_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we want to create another instance with specific configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// build instance another spark instance manually \n",
    "val sparkSession = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val bikeData1 = sparkSession.read.option(\"header\", \"true\").csv(\"datalake/bike-data/201508_trip_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame` is the main class that we use to do transformation with our data. We can asume it like a table in relational database. When we load a datasource actually we can say it's a table. It can be joined with another and also doing other operations like `select()`, `filter()`, `sum()`, `max()`, `groupBy()`, etc. In the example above, `bikeData` and `bikeData1` are instance of `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// doing operation to DataFrame\n",
    "bikeData.filter($\"`Duration`\" > 1000)\n",
    "    .select(\n",
    "        $\"`Trip ID`\".as(\"trip_id\"),\n",
    "        $\"`Duration`\".as(\"duration\"),\n",
    "        $\"`Start Station`\".as(\"start_station\"),\n",
    "        $\"`End Station`\".as(\"end_station\")\n",
    "    )\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are another table like data class in `Spark` called `DataSet`. The main difference between `DataFrame` and `DataSet`: `DataSet` is `strongly-typed` data and `DataFrame` is `untyped-data`. Please read this [reference](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html) to get more explanation about `DataFrame` and `DataSet` comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame` operations in `Spark` actually very similar with operations that exists in `SQL`. If we already familiar with `SQL` syntax, we can easily getting familiar with `Spark`. We just need to understand the equivalent operation/function that exist in `Spark`. Here are some common operation that we usually use in `SQL` for transform our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasource files to `Spark` `DataFrame`. Datasource format can be `csv`, `json`, `parquet`, etc. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val datalakeSrc = \"datalake/business_platform\"\n",
    "val salesDeliverySrc = spark.read.parquet(s\"$datalakeSrc/sales_delivery\") // <-- as from explanation above, this will return a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the structure of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// inspect schema\n",
    "salesDeliverySrc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val salesDeliverySrc = spark.read.parquet(s\"$datalakeSrc/sales_delivery\")\n",
    "\n",
    "salesDeliverySrc\n",
    "    .select(\n",
    "        $\"`sales.sales_delivery.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        $\"`sales.sales_delivery.additional_data.locale`\".as(\"locale\"),\n",
    "        $\"`sales.sales_delivery.sales_delivery_id`\".as(\"sales_delivery_id\"),\n",
    "        $\"`sales.sales_delivery.total_amount`\".as(\"total_amount\")\n",
    "    )\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent with `WHERE` syntax in `SQL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val salesDeliverySrc = spark.read.parquet(s\"$datalakeSrc/sales_delivery\")\n",
    "\n",
    "salesDeliverySrc\n",
    "    .filter(\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\" === \"CULINARY\"\n",
    "    )\n",
    "    .select(\n",
    "        $\"`sales.sales_delivery.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        $\"`sales.sales_delivery.additional_data.locale`\".as(\"locale\"),\n",
    "        $\"`sales.sales_delivery.sales_delivery_id`\".as(\"sales_delivery_id\"),\n",
    "        $\"`sales.sales_delivery.total_amount`\".as(\"total_amount\"),\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\".as(\"trip_type\")\n",
    "    )\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some rows in `DataFrame`, returning an `Array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val salesDeliverySrc = spark.read.parquet(s\"$datalakeSrc/sales_delivery\")\n",
    "\n",
    "salesDeliverySrc\n",
    "    .filter(\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\" === \"CULINARY\"\n",
    "    )\n",
    "    .select(\n",
    "        $\"`sales.sales_delivery.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        $\"`sales.sales_delivery.additional_data.locale`\".as(\"locale\"),\n",
    "        $\"`sales.sales_delivery.sales_delivery_id`\".as(\"sales_delivery_id\"),\n",
    "        $\"`sales.sales_delivery.total_amount`\".as(\"total_amount\"),\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\".as(\"trip_type\")\n",
    "    )\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining two or more DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load Data\n",
    "val salesDeliverySrc = spark.read.parquet(s\"$datalakeSrc/sales_delivery\")\n",
    "val salesInvoiceSrc = spark.read.parquet(s\"$datalakeSrc/sales_invoice\")\n",
    "\n",
    "// Do some operation\n",
    "val salesDelivery = salesDeliverySrc\n",
    "    .filter(\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\" === \"CULINARY\"\n",
    "    )\n",
    "    .select(\n",
    "        $\"`sales.sales_delivery.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        $\"`sales.sales_delivery.additional_data.locale`\".as(\"locale\"),\n",
    "        $\"`sales.sales_delivery.sales_delivery_id`\".as(\"sales_delivery_id\"),\n",
    "        $\"`sales.sales_delivery.total_amount`\".as(\"total_amount\"),\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\".as(\"trip_type\")\n",
    "    )\n",
    "    .as(\"bp_sales_delivery\")\n",
    "\n",
    "val salesInvoice = salesInvoiceSrc\n",
    "    .filter(\n",
    "        $\"`sales.sales_invoice.additional_data.tripType`\" === \"CULINARY\"\n",
    "    )\n",
    "    .select(\n",
    "        $\"`sales.sales_invoice.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        $\"`sales.sales_invoice.sales_invoice_id`\".as(\"sales_invoice_id\")    \n",
    "    )\n",
    "    .as(\"bp_sales_invoice\")\n",
    "\n",
    "// Join data\n",
    "salesDelivery\n",
    "    .join(salesInvoice, $\"bp_sales_delivery.booking_id\" === $\"bp_sales_invoice.booking_id\", \"left\")\n",
    "    .select(\n",
    "        $\"*\"\n",
    "    )\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when() and otherwise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent to `CASE...WHEN` syntax in `SQL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{when, lit} // dont forget to import the function\n",
    "\n",
    "val salesDeliverySrc = spark.read.parquet(s\"$datalakeSrc/sales_delivery\")\n",
    "\n",
    "salesDeliverySrc\n",
    "    .filter(\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\" === \"CULINARY\"\n",
    "    )\n",
    "    .select(\n",
    "        $\"`sales.sales_delivery.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        when($\"`sales.sales_delivery.additional_data.locale`\" === \"id_ID\", lit(\"ID\"))\n",
    "            .otherwise(lit(\"Non ID\")).as(\"locale\"),\n",
    "        $\"`sales.sales_delivery.sales_delivery_id`\".as(\"sales_delivery_id\"),\n",
    "        $\"`sales.sales_delivery.total_amount`\".as(\"total_amount\"),\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\".as(\"trip_type\")\n",
    "    )\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupby() and agg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent to `GROUP` BY in `SQL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val salesDeliverySrc = spark.read.parquet(s\"$datalakeSrc/sales_delivery\")\n",
    "\n",
    "salesDeliverySrc\n",
    "    .filter(\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\" === \"CULINARY\"\n",
    "    )\n",
    "    .select(\n",
    "        $\"`sales.sales_delivery.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        $\"`sales.sales_delivery.additional_data.locale`\".as(\"locale\"),\n",
    "        $\"`sales.sales_delivery.sales_delivery_id`\".as(\"sales_delivery_id\"),\n",
    "        $\"`sales.sales_delivery.total_amount`\".as(\"total_amount\"),\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\".as(\"trip_type\")\n",
    "    )\n",
    "    .groupBy($\"locale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent to `MAX` in `SQL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.max // dont forget to import the function\n",
    "\n",
    "val salesDeliverySrc = spark.read.parquet(s\"$datalakeSrc/sales_delivery\")\n",
    "\n",
    "salesDeliverySrc\n",
    "    .filter(\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\" === \"CULINARY\"\n",
    "    )\n",
    "    .select(\n",
    "        $\"`sales.sales_delivery.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        $\"`sales.sales_delivery.additional_data.locale`\".as(\"locale\"),\n",
    "        $\"`sales.sales_delivery.sales_delivery_id`\".as(\"sales_delivery_id\"),\n",
    "        $\"`sales.sales_delivery.total_amount`\".as(\"total_amount\"),\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\".as(\"trip_type\")\n",
    "    )\n",
    "    .groupBy($\"locale\")\n",
    "    .agg(\n",
    "        max($\"total_amount\").as(\"max_total_amount\")\n",
    "    )\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent to `SUM` in `SQL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.sum \n",
    "\n",
    "val salesDeliverySrc = spark.read.parquet(s\"$datalakeSrc/sales_delivery\")\n",
    "\n",
    "val salesDelivery = salesDeliverySrc\n",
    "    .filter(\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\" === \"CULINARY\"\n",
    "    )\n",
    "    .select(\n",
    "        $\"`sales.sales_delivery.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        $\"`sales.sales_delivery.additional_data.locale`\".as(\"locale\"),\n",
    "        $\"`sales.sales_delivery.sales_delivery_id`\".as(\"sales_delivery_id\"),\n",
    "        $\"`sales.sales_delivery.total_amount`\".as(\"total_amount\"),\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\".as(\"trip_type\")\n",
    "    )\n",
    "    .groupBy($\"locale\")\n",
    "    .agg(\n",
    "        sum($\"total_amount\").as(\"sum_total_amount\")\n",
    "    )\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicated rows result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val salesDeliverySrc = spark.read.parquet(s\"$datalakeSrc/sales_delivery\")\n",
    "\n",
    "// Do some operation\n",
    "val salesDelivery = salesDeliverySrc\n",
    "    .filter(\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\" === \"CULINARY\"\n",
    "    )\n",
    "    .select(\n",
    "        $\"`sales.sales_delivery.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        $\"`sales.sales_delivery.additional_data.locale`\".as(\"locale\"),\n",
    "        $\"`sales.sales_delivery.sales_delivery_id`\".as(\"sales_delivery_id\"),\n",
    "        $\"`sales.sales_delivery.total_amount`\".as(\"total_amount\"),\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\".as(\"trip_type\")\n",
    "    )\n",
    "    .dropDuplicates()\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explain()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "See query execution plan. Equievalent to `EXPLAIN` in `SQL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load Data\n",
    "val salesDeliverySrc = spark.read.parquet(s\"$datalakeSrc/sales_delivery\")\n",
    "val salesInvoiceSrc = spark.read.parquet(s\"$datalakeSrc/sales_invoice\")\n",
    "\n",
    "// Do some operation\n",
    "val salesDelivery = salesDeliverySrc\n",
    "    .filter(\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\" === \"CULINARY\"\n",
    "    )\n",
    "    .select(\n",
    "        $\"`sales.sales_delivery.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        $\"`sales.sales_delivery.additional_data.locale`\".as(\"locale\"),\n",
    "        $\"`sales.sales_delivery.sales_delivery_id`\".as(\"sales_delivery_id\"),\n",
    "        $\"`sales.sales_delivery.total_amount`\".as(\"total_amount\"),\n",
    "        $\"`sales.sales_delivery.additional_data.tripType`\".as(\"trip_type\")\n",
    "    )\n",
    "    .as(\"bp_sales_delivery\")\n",
    "\n",
    "val salesInvoice = salesInvoiceSrc\n",
    "    .filter(\n",
    "        $\"`sales.sales_invoice.additional_data.tripType`\" === \"CULINARY\"\n",
    "    )\n",
    "    .select(\n",
    "        $\"`sales.sales_invoice.additional_data.bookingId`\".as(\"booking_id\"),\n",
    "        $\"`sales.sales_invoice.sales_invoice_id`\".as(\"sales_invoice_id\")    \n",
    "    )\n",
    "    .as(\"bp_sales_invoice\")\n",
    "\n",
    "// Join data\n",
    "salesDelivery\n",
    "    .join(salesInvoice, $\"bp_sales_delivery.booking_id\" === $\"bp_sales_invoice.booking_id\", \"left\")\n",
    "    .select(\n",
    "        $\"*\"\n",
    "    )\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Lazy evaulation` means that `Spark` will wait until the very last moment to execute the graph of computation instructions. In `Spark`, instead of modifying the data immediately when you express some operation, we build up a plan of transformations that you would like to apply to your source data. By waiting until the last minute to execute the code, `Spark` compiles this plan from our raw `DataFrame` transformations to a streamlined physical plan that will run as efficiently as possible across the cluster. So on the examples above, any transform operation is not executed until `show()` function getting called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
